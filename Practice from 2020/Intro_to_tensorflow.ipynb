{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to tensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeXdYGhLTYtbwnyFSGIbEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CraigTLennon/Tensorflow/blob/master/Intro_to_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdCEA5F51XOH",
        "colab_type": "text"
      },
      "source": [
        "Installation of tensorflow based on getting started in tensorflow tutorial\n",
        "Below, ! means run as a shell command rather than a notebook command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2WkBGAc1SMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### !pip install tensorflow Not executed because I already have it installed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uGHd6NY22Qt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c115187-81e3-4ee3-a6a8-bb5b538dc9b3"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__) ##check to see if installed"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-6a4ySx3ZK_",
        "colab_type": "text"
      },
      "source": [
        "For a gpu version of tensorflow, reset the runtime, then set the runtime type to GPU, and pip install with gpu (below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SeuV_J7X4IG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "7932765e-21ba-4c4a-eaf9-8cb9d075ec1b"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.28.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (46.1.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2020.4.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=4bdff4a34a60ba6233bfaed221e64c9198fa01f79127626cff35d36ff112cb3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc3 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EApjliR34hez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1372872a-0e9c-47c8-e3ee-f791766d8395"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_xpGLuj64lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH1nxkXD7DYr",
        "colab_type": "text"
      },
      "source": [
        "We start with data froma linear function, $y=2x-1$, and a model which consists of a single unit, taking in one value.  Then we specify a loss funtion and an optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OPX0VB38Mno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=np.array([-1.0,0.0,1.0,2.0,3.0,4.0], dtype=float)\n",
        "y=np.array([-3.0,-1.0,1.0,3.0,5.0,7.0], dtype=float)\n",
        "model=tf.keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])\n",
        "model.compile(optimizer='sgd',loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p64_if6j9MT5",
        "colab_type": "text"
      },
      "source": [
        "Then we fit the model, running for 500 epochs before asking it to predict a value for x=10.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug_D91EC9XD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "926cb5ba-841a-4da6-ac79-c2ae1c43b1d8"
      },
      "source": [
        "model.fit(x,y,epochs=500)\n",
        "print(model.predict([10.0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 27.2687\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 925us/step - loss: 21.7544\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 17.4099\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 13.9857\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 11.2859\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.1559\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 7.4745\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1461\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.0955\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.2636\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.6039\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0797\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6623\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3290\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.0620\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.8471\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6735\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5324\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4169\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3217\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.2426\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1762\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.1198\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.0715\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0296\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9928\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9601\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9306\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.9039\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.8793\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8565\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8352\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.8151\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7961\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7779\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7605\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7438\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7276\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.7120\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6968\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6821\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.6677\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6538\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6401\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6268\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6138\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.6011\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5887\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.5765\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5646\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5530\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5416\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5304\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5195\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.5088\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4984\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4881\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4781\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4683\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4586\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4492\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4400\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4310\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4221\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.4134\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.4049\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3966\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3885\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3805\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3727\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3650\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3575\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3502\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3430\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3359\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3290\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3223\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3157\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.3092\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.3028\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2966\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2905\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2845\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2787\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2730\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2674\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2619\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2565\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2512\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2461\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2410\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2361\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2312\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2265\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2218\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2173\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2128\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2084\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.2041\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1999\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1958\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1918\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1879\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1840\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1802\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1765\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1729\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1694\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1659\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1625\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1591\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1559\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1527\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1495\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1465\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1435\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1405\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1376\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1348\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1320\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1293\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1267\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1241\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1215\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1190\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1166\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1142\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1118\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1095\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1073\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1051\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1029\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1008\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0987\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0967\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0947\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0928\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0909\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0890\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0872\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0854\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0836\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0819\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0802\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0786\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0770\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0754\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0738\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0723\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0708\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0694\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0680\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0666\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0652\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0639\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0625\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0613\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0600\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0588\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0576\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0564\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0552\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0541\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0530\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0519\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0508\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0498\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0488\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0478\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0468\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0458\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0449\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0439\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0430\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0422\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0413\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0404\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0396\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0388\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0380\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0372\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0365\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0357\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0350\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0343\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0336\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0329\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0322\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0315\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0309\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0302\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0296\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0290\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0284\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0278\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0273\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0267\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0262\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0256\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0251\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0246\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0241\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0236\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0231\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0226\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0222\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0217\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0213\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0208\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0204\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0200\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0188\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0184\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0180\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0176\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0173\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0162\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0159\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0149\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0143\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0135\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0132\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0124\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0121\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0119\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0116\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0114\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0112\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0109\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0107\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0105\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0103\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0101\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0099\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0097\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0095\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0093\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0091\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0089\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0087\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0085\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0084\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0082\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0080\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0078\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0077\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0075\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0074\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0072\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0071\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0069\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0068\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0066\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0062\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0061\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0060\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0059\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0057\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0056\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0055\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0054\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0053\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0052\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0051\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0050\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0049\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0048\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0047\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0046\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0045\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0044\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0043\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0042\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0041\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0040\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0040\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0039\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0037\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0036\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0036\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0035\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0034\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0033\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0032\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0031\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0030\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0028\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0026\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0025\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0024\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0021\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0020\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0019\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0019\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0018\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0018\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0017\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0017\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0016\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0015\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0014\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0013\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0012\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0011\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0010\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.8503e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.6479e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.4498e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.2557e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0655e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 8.8793e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6969e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.5183e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3433e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.1719e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 8.0041e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8397e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.6787e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.5209e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.3664e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.2151e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.0669e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.9217e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.7796e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.6403e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.5039e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.3703e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2395e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.1113e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9858e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.8629e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7424e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.6245e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.5090e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3958e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.2850e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.1764e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0701e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.9660e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.8640e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.7640e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.6662e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.5704e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.4765e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.3845e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2945e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.2062e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 4.1198e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 4.0352e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.9523e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.8711e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.7916e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 3.7137e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 3.6375e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.5627e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4896e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.4179e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.3477e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2789e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.2115e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.1456e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0810e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 3.0177e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.9557e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8950e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.8355e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.7773e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.7202e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6644e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.6096e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5560e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.5035e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4521e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.4017e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.3524e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.3041e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.2568e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.2104e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.1650e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.1205e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 2.0770e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 2.0343e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9925e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.9516e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.9115e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8722e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.8338e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.7961e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.7592e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.7231e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6877e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6530e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.6191e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 1.5858e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.5532e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.5213e-04\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4901e-04\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4595e-04\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.4295e-04\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.4001e-04\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 1.3714e-04\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3432e-04\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.3156e-04\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2886e-04\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 926us/step - loss: 1.2621e-04\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2362e-04\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.2108e-04\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1859e-04\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1616e-04\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1377e-04\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.1143e-04\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0914e-04\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0690e-04\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0471e-04\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0255e-04\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 1.0045e-04\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.8384e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.6364e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.4384e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 9.2446e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 9.0546e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.8687e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.6865e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.5082e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.3335e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 8.1623e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.9946e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.8305e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.6695e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.5120e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.3577e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 7.2067e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 7.0586e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.9135e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.7716e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.6325e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.4962e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.3629e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 6.2322e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 6.1042e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.9788e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.8560e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.7358e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.6179e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.5025e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.3895e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 5.2787e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 5.1703e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 5.0642e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 4.9601e-05\n",
            "[[18.979452]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTGF7XWj9jwA",
        "colab_type": "text"
      },
      "source": [
        "It predicts roughly 19, but not exactly.\n",
        "\n",
        "Moving on to a more complex dataset, we try the fashion mnist database, which is a set of images of clothing intended to replace the old mnist digits set as a canonical training set.  This dataset happens to be in the keras package and can be loaded directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "604eXnn2GJ2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "da8e53f9-c649-4075-a4a3-33e30c58b6e2"
      },
      "source": [
        "fashion=keras.datasets.fashion_mnist\n",
        "(train_images, train_labels),(test_images,test_labels)=fashion.load_data()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tGLGwJ9JpaL",
        "colab_type": "text"
      },
      "source": [
        "Next we scale our images so that the pixel values lie between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvVhYs1IJx9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images=train_images/255.0\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0P_byCHa0l",
        "colab_type": "text"
      },
      "source": [
        "Our model will take the 28 x 28 image and flatten it into a 1 dimensional array, followed by a fully connected middle layer with 128 neurons, and an output layer with one output neuron  per class.  The middle layer has a relu activation function, while the output layer is softmax, which selects the maximum of output values.\n",
        " In the command below, sequential defines a sequence of layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4GWIY7iH_B8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "7fe89d98-695a-4899-a938-3588997fb30c"
      },
      "source": [
        "fashion_model=keras.Sequential([keras.layers.Flatten(input_shape=(28,28)),keras.layers.Dense(256,activation=tf.nn.relu),keras.layers.Dense(10,activation=tf.nn.softmax) ])\n",
        "fashion_model.compile(optimizer=tf.keras.optimizers.Adam(),loss='sparse_categorical_crossentropy')\n",
        "fashion_model.fit(train_images,train_labels,epochs=15)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4819\n",
            "Epoch 2/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3645\n",
            "Epoch 3/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3281\n",
            "Epoch 4/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3043\n",
            "Epoch 5/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2853\n",
            "Epoch 6/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2719\n",
            "Epoch 7/15\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2592\n",
            "Epoch 8/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2486\n",
            "Epoch 9/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2381\n",
            "Epoch 10/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2296\n",
            "Epoch 11/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2182\n",
            "Epoch 12/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2132\n",
            "Epoch 13/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2060\n",
            "Epoch 14/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1978\n",
            "Epoch 15/15\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1904\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f663b2c2550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOa3OwcGN22K",
        "colab_type": "text"
      },
      "source": [
        "We can also evaluate our model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsudpEORN7_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "49016e47-9f52-4a75-e6c6-c495c48e4fec"
      },
      "source": [
        "fashion_model.evaluate(test_images,test_labels)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 0.3308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33079448342323303"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z2KVuPlP2Wj",
        "colab_type": "text"
      },
      "source": [
        "Callbacks are functions which do something after some other function is executed (call me back after...)  Below we create one to stop training when accuracy reasches 65%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnOcbOIfQNFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "15ee6400-9644-49b6-f811-72af39d46758"
      },
      "source": [
        "class tf_call_back(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('loss')<.4):\n",
        "      print(\"\\n done\")\n",
        "      self.model.stop_training=True\n",
        "callback=tf_call_back()\n",
        "fashion_model.fit(train_images,train_labels,epochs=15,callbacks=[callback])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.1338\n",
            " done\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66335eea90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huIxt_Ht4R0r",
        "colab_type": "text"
      },
      "source": [
        "Convolutional Neural Networks preserve locality by averaging or filtering nearby cells to break the image into set of features, such as similar areas, edges, etc.  Then they reduce size by pooling those layers, leaving only information about the maximum value within the region.    In the model below, the first layer is a 3X3 convolution, followed by max pooling over squares of size 4, then another convolution and pooling, after which the image is flattened and run through a fully connected layer before prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZLYSNaE5j23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_model=tf.keras.models.Sequential([\n",
        "                                      tf.keras.layers.Conv2D(64,(3,3), activation='relu', input_shape=(28,28,1)),\n",
        "                                       tf.keras.layers.MaxPool2D(2,2),\n",
        "                                       tf.keras.layers.Conv2D(65,(3,3),activation='relu'),\n",
        "                                       tf.keras.layers.MaxPool2D(2,2),\n",
        "                                       tf.keras.layers.Flatten(),\n",
        "                                       tf.keras.layers.Dense(128,activation='relu'),\n",
        "                                       tf.keras.layers.Dense(10,activation='softmax')                                   \n",
        "                                       ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trzEL0ra7iiT",
        "colab_type": "text"
      },
      "source": [
        "The model summary method will break out the size of the image as it goes through each layer, along with the number of parameters associated with the layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk7qYQ5R7u1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "04d9360d-1a4f-44f5-fa58-b6fd67b44b9e"
      },
      "source": [
        "conv_model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 11, 11, 65)        37505     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 65)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1625)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               208128    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 247,563\n",
            "Trainable params: 247,563\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d--vDBGH9Fyk",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b73JoPT29oAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion=keras.datasets.fashion_mnist\n",
        "(train_images, train_labels),(test_images,test_labels)=fashion.load_data()\n",
        "train_images=train_images.reshape(60000,28,28,1)\n",
        "test_images=test_images.reshape(10000,28,28,1)\n",
        "train_images=train_images/255.0\n",
        "test_images=test_images/255.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4yM0dNF-BRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1650458d-976e-4f54-e975-c9c92b31114d"
      },
      "source": [
        "conv_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy')\n",
        "conv_model.fit(train_images,train_labels,epochs=5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 90s 48ms/step - loss: 0.4378\n",
            "Epoch 2/5\n",
            " 273/1875 [===>..........................] - ETA: 1:17 - loss: 0.3204\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-223eb39458b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    856\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mbatch_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mbatch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0madd_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_steps\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0madd_seen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    587\u001b[0m       \u001b[0mprev_total_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\b'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    398\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    399\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}